{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-02T05:28:08.793601Z",
          "start_time": "2018-03-02T05:28:08.779176Z"
        },
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "EUuIHF1FYLXF"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import math\n",
        "import csv\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, merge, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, CSVLogger\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from PIL import Image\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt # to plot images\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEPvRDvHYLXG"
      },
      "source": [
        "### Consts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-02T05:20:58.013787Z",
          "start_time": "2018-03-02T05:20:58.010973Z"
        },
        "id": "G9lzRkjaYLXH"
      },
      "outputs": [],
      "source": [
        "train_and_val_dataset_file = 'datasets/train-and-val.pkl'\n",
        "test_dataset_file = 'datasets/test.pkl'\n",
        "saved_model_filename = \"datasets/test-4-new-tentative-{epoch:02d}-{val_dice_coef_accur:.4f}.hdf5\"\n",
        "csv_logger_training = \"datasets/test-4-new-tentative.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-06-28T15:25:33.893434",
          "start_time": "2017-06-28T15:25:33.886889"
        },
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "qSvOhDHlYLXI"
      },
      "source": [
        "### Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-02T05:21:02.968954Z",
          "start_time": "2018-03-02T05:21:00.250945Z"
        },
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "dt31MXM6YLXI"
      },
      "outputs": [],
      "source": [
        "X_remaining, Y_remaining, remaining_dataset_desc = joblib.load(train_and_val_dataset_file)\n",
        "Xte, yte, test_dataset_desc = joblib.load(test_dataset_file) # X and y for test\n",
        "training_set_index = remaining_dataset_desc['training_set_index']\n",
        "validation_set_index = remaining_dataset_desc['validation_set_index']\n",
        "\n",
        "Xva, yva = X_remaining[training_set_index:validation_set_index,:], Y_remaining[training_set_index:validation_set_index] # X and y for validation\n",
        "Xtr, ytr = joblib.load(\"datasets/train-augmented-11216.pkl\")\n",
        "\n",
        "print(Xtr.shape)\n",
        "print(Xva.shape)\n",
        "print(Xte.shape)\n",
        "print(ytr.shape)\n",
        "print(yva.shape)\n",
        "print(yte.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-07-09T14:52:12.718813Z",
          "start_time": "2017-07-09T14:52:11.827824Z"
        },
        "id": "ET4PN4S_YLXI"
      },
      "source": [
        "### Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-02T05:21:24.155378Z",
          "start_time": "2018-03-02T05:21:19.322487Z"
        },
        "id": "J1mQ8ZcEYLXI"
      },
      "outputs": [],
      "source": [
        "# Preprocessing in the training set (mean and sd) and apply it to all sets\n",
        "\n",
        "full_image_mean_value = Xtr.mean() # mean-value for each pixel of all full images\n",
        "full_image_sd = Xtr.std() # standard deviation for each pixel of all full images\n",
        "\n",
        "Xtr = (Xtr - full_image_mean_value) / full_image_sd\n",
        "Xva = (Xva - full_image_mean_value) / full_image_sd\n",
        "Xte = (Xte - full_image_mean_value) / full_image_sd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phiQx8MZYLXJ"
      },
      "source": [
        "### Pre-configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-02T05:21:31.205219Z",
          "start_time": "2018-03-02T05:21:31.200237Z"
        },
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "uEJU6rr6YLXJ"
      },
      "outputs": [],
      "source": [
        "K.set_image_data_format('channels_last')  # TF dimension\n",
        "_, *input_image_shape, _ = Xtr.shape\n",
        "input_image_shape = tuple(input_image_shape)\n",
        "print(input_image_shape)\n",
        "\n",
        "smooth = 1.\n",
        "\n",
        "use_dropout = True\n",
        "use_regularizers = True\n",
        "dropout_rate = 0.5\n",
        "number_of_epochs = 1000\n",
        "batch_size = 64\n",
        "kernel_size = (5, 5)\n",
        "initial_volume_size = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-06-30T18:07:40.403398",
          "start_time": "2017-06-30T18:07:40.398392"
        },
        "id": "EVOebE4qYLXJ"
      },
      "source": [
        "### Define Unet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-03-02T05:28:33.082757Z",
          "start_time": "2018-03-02T05:28:31.096328Z"
        },
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "uB-MoumaYLXK"
      },
      "outputs": [],
      "source": [
        "# Define loss function\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = 2 * K.sum(y_true_f * y_pred_f) + smooth\n",
        "    union = K.sum(y_true_f) + K.sum(y_pred_f) + smooth\n",
        "    return K.mean(intersection / union)\n",
        "\n",
        "def dice_coef_per_image_in_batch(y_true, y_pred):\n",
        "    y_true_f = K.batch_flatten(y_true)\n",
        "    y_pred_f = K.batch_flatten(y_pred)\n",
        "    intersection = 2. * K.sum(y_true_f * y_pred_f, axis=1, keepdims=True) + smooth\n",
        "    union = K.sum(y_true_f, axis=1, keepdims=True) + K.sum(y_pred_f, axis=1, keepdims=True) + smooth\n",
        "    return K.mean(intersection / union)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return -dice_coef_per_image_in_batch(y_true, y_pred)\n",
        "\n",
        "def dice_coef_accur(y_true, y_pred):\n",
        "    return dice_coef_per_image_in_batch(y_true, y_pred)\n",
        "\n",
        "def IOU_calc(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "\n",
        "    return 2*(intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def IOU_calc_loss(y_true, y_pred):\n",
        "    return -IOU_calc(y_true, y_pred)\n",
        "\n",
        "def setup_regularizers(conv_layer):\n",
        "    return BatchNormalization()(conv_layer) if use_regularizers else conv_layer\n",
        "\n",
        "def setup_dropout(conv_layer):\n",
        "    return Dropout(dropout_rate)(conv_layer) if use_dropout else conv_layer\n",
        "\n",
        "# Define model\n",
        "inputs = Input((*input_image_shape, 1))\n",
        "conv1 = Conv2D(initial_volume_size, kernel_size, activation='relu', padding='same')(inputs)\n",
        "conv1 = Conv2D(initial_volume_size, kernel_size, activation='relu', padding='same')(conv1)\n",
        "conv1 = setup_regularizers(conv1)\n",
        "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "conv2 = Conv2D(initial_volume_size*2, kernel_size, activation='relu', padding='same')(pool1)\n",
        "conv2 = Conv2D(initial_volume_size*2, kernel_size, activation='relu', padding='same')(conv2)\n",
        "conv2 = setup_regularizers(conv2)\n",
        "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "conv3 = Conv2D(initial_volume_size*4, kernel_size, activation='relu', padding='same')(pool2)\n",
        "conv3 = Conv2D(initial_volume_size*4, kernel_size, activation='relu', padding='same')(conv3)\n",
        "conv3 = setup_regularizers(conv3)\n",
        "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "conv4 = Conv2D(initial_volume_size*8, kernel_size, activation='relu', padding='same')(pool3)\n",
        "conv4 = Conv2D(initial_volume_size*8, kernel_size, activation='relu', padding='same')(conv4)\n",
        "conv4 = setup_regularizers(conv4)\n",
        "pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)tf.keras.optimizers.schedules.ExponentialDecay\n",
        "\n",
        "conv5 = Conv2D(initial_volume_size*16, kernel_size, activation='relu', padding='same')(pool4)\n",
        "conv5 = Conv2D(initial_volume_size*16, kernel_size, activation='relu', padding='same')(conv5)\n",
        "conv5 = setup_regularizers(conv5)\n",
        "\n",
        "up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=3)\n",
        "up6 = setup_dropout(up6)\n",
        "conv6 = Conv2D(initial_volume_size*8, kernel_size, activation='relu', padding='same')(up6)\n",
        "conv6 = Conv2D(initial_volume_size*8, kernel_size, activation='relu', padding='same')(conv6)\n",
        "\n",
        "up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=3)\n",
        "up7 = setup_dropout(up7)\n",
        "conv7 = Conv2D(initial_volume_size*4, kernel_size, activation='relu', padding='same')(up7)\n",
        "conv7 = Conv2D(initial_volume_size*4, kernel_size, activation='relu', padding='same')(conv7)\n",
        "\n",
        "up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=3)\n",
        "up8 = setup_dropout(up8)\n",
        "conv8 = Conv2D(initial_volume_size*2, kernel_size, activation='relu', padding='same')(up8)\n",
        "conv8 = Conv2D(initial_volume_size*2, kernel_size, activation='relu', padding='same')(conv8)\n",
        "\n",
        "up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=3)\n",
        "up9 = setup_dropout(up9)\n",
        "conv9 = Conv2D(initial_volume_size, kernel_size, activation='relu', padding='same')(up9)\n",
        "conv9 = Conv2D(initial_volume_size, kernel_size, activation='relu', padding='same')(conv9)\n",
        "\n",
        "conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[conv10])\n",
        "\n",
        "\n",
        "initial_learning_rate = 1e-5\n",
        "lr_schedule = ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=2000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "model.compile(optimizer=Adam(lr=lr_schedule), loss=dice_coef_loss, metrics=[dice_coef_accur])\n",
        "print(\"Size of the CNN: %s\" % model.count_params())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gYv70rWYLXK"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ0rlxJdYLXL"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2017-11-20T22:25:14.582Z"
        },
        "run_control": {
          "frozen": false,
          "read_only": false
        },
        "id": "q-SoW23DYLXL"
      },
      "outputs": [],
      "source": [
        "# Define callbacks\n",
        "model_checkpoint = ModelCheckpoint(saved_model_filename, monitor='val_dice_coef_accur', save_best_only=True, verbose=1)\n",
        "csv_logger = CSVLogger(csv_logger_training, append=True, separator=';')\n",
        "\n",
        "# Train\n",
        "history = model.fit(Xtr, ytr, batch_size=batch_size, epochs=number_of_epochs, verbose=1, shuffle=True,\n",
        "             callbacks=[model_checkpoint, csv_logger], validation_data=(Xva, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJEtl25FYLXL"
      },
      "source": [
        "### Show training metrics / loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N_8xCneYLXL"
      },
      "outputs": [],
      "source": [
        "csv_history_file = \"datasets/test-4-new-tentative.csv\"\n",
        "data = {}\n",
        "\n",
        "with open(csv_history_file, \"r\") as f:\n",
        "    reader = csv.reader(f, delimiter=\";\")\n",
        "    for i, line in enumerate(reader):\n",
        "        if i > 0:\n",
        "            dice_coef_accur_in_csv, loss_in_csv, val_dice_coef_accur_in_csv, val_loss_in_csv = float(line[1]), float(line[2]), float(line[3]), float(line[4])\n",
        "\n",
        "            dice_coef_accur_list = data.get('dice_coef_accur', None)\n",
        "            if dice_coef_accur_list is None:\n",
        "                dice_coef_accur_list = [dice_coef_accur_in_csv]\n",
        "                data['dice_coef_accur'] = dice_coef_accur_list\n",
        "            else:\n",
        "                data['dice_coef_accur'].append(dice_coef_accur_in_csv)\n",
        "\n",
        "            loss_list = data.get('loss', None)\n",
        "            if loss_list is None:\n",
        "                loss_list = [loss_in_csv]\n",
        "                data['loss'] = loss_list\n",
        "            else:\n",
        "                data['loss'].append(loss_in_csv)\n",
        "\n",
        "            val_dice_coef_accur_list = data.get('val_dice_coef_accur', None)\n",
        "            if val_dice_coef_accur_list is None:\n",
        "                val_dice_coef_accur_list = [val_dice_coef_accur_in_csv]\n",
        "                data['val_dice_coef_accur'] = val_dice_coef_accur_list\n",
        "            else:\n",
        "                data['val_dice_coef_accur'].append(val_dice_coef_accur_in_csv)\n",
        "\n",
        "            val_loss_list = data.get('val_loss', None)\n",
        "            if val_loss_list is None:\n",
        "                val_loss_list = [val_loss_in_csv]\n",
        "                data['val_loss'] = val_loss_list\n",
        "            else:\n",
        "                data['val_loss'].append(val_loss_in_csv)\n",
        "\n",
        "x = data['dice_coef_accur']\n",
        "y = data['val_dice_coef_accur']\n",
        "plt.plot(x, label='train')\n",
        "plt.plot(y, label = 'val')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "x = data['loss']\n",
        "y = data['val_loss']\n",
        "plt.plot(x[:300], label='train')\n",
        "plt.plot(y[:300], label='val')\n",
        "plt.title(\"Training and validation loss over epochs\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "plt.savefig(\"Training_loss.jpg\", dpi=1200,  bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnWBV7D2YLXM"
      },
      "source": [
        "### Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuf8wshqYLXM"
      },
      "source": [
        "### Predict masks using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-28T02:10:52.814793Z",
          "start_time": "2017-11-28T02:10:27.358370Z"
        },
        "id": "OY3oaN6jYLXM"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"datasets/test-4-new-tentative-821-0.9578.hdf5\")\n",
        "imgs_mask_test = model.predict(Xte, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3xL3oXWYLXM"
      },
      "outputs": [],
      "source": [
        "print(np.median(imgs_mask_test[0]))\n",
        "acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "acc_metric.update_state(imgs_mask_test, yte)\n",
        "print(\"Accuracy: \", acc_metric.result().numpy())\n",
        "\n",
        "meaniou_metric = tf.keras.metrics.MeanIoU(2)\n",
        "meaniou_metric.update_state(imgs_mask_test, yte)\n",
        "print(\"MeanIOU: \", meaniou_metric.result().numpy())\n",
        "\n",
        "recall_metric = tf.keras.metrics.Recall()\n",
        "recall_metric.update_state(imgs_mask_test, yte)\n",
        "print(\"Recall / Sensitivity: \", recall_metric.result().numpy())\n",
        "\n",
        "precision_metric = tf.keras.metrics.Precision()\n",
        "precision_metric.update_state(imgs_mask_test, yte)\n",
        "print(\"Precision: \", precision_metric.result().numpy())\n",
        "\n",
        "tp_metric = tf.keras.metrics.TruePositives()\n",
        "tp_metric.update_state(imgs_mask_test, yte)\n",
        "tp = tp_metric.result().numpy()\n",
        "print(\"TP: \", tp)\n",
        "\n",
        "tn_metric = tf.keras.metrics.TrueNegatives()\n",
        "tn_metric.update_state(imgs_mask_test, yte)\n",
        "tn = tn_metric.result().numpy()\n",
        "print(\"TN: \", tn)\n",
        "\n",
        "fp_metric = tf.keras.metrics.FalsePositives()\n",
        "fp_metric.update_state(imgs_mask_test, yte)\n",
        "fp = fp_metric.result().numpy()\n",
        "print(\"FP: \", fp)\n",
        "\n",
        "fn_metric = tf.keras.metrics.FalseNegatives()\n",
        "fn_metric.update_state(imgs_mask_test, yte)\n",
        "fn = fn_metric.result().numpy()\n",
        "print(\"FN: \", fn)\n",
        "\n",
        "print(\"Specificity: \", tn/(fp+tn))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-06-30T18:13:17.713578",
          "start_time": "2017-06-30T18:13:17.708972"
        },
        "id": "yoQK1cGxYLXM"
      },
      "source": [
        "### Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-11-28T02:11:07.728776Z",
          "start_time": "2017-11-28T02:11:01.346361Z"
        },
        "id": "_76Fs-ccYLXM"
      },
      "outputs": [],
      "source": [
        "ncols = 3 # number of columns in final grid of images\n",
        "nrows = 8 # looking at all images takes some time\n",
        "_, axes = plt.subplots(nrows, ncols, figsize=(17, 17*nrows/ncols))\n",
        "for axis in axes.flatten():\n",
        "    axis.set_axis_off()\n",
        "    axis.set_aspect('equal')\n",
        "\n",
        "for k in range(0, nrows):\n",
        "    im_test_original = Xte[k].reshape(*input_image_shape)\n",
        "    im_result = imgs_mask_test[k].reshape(*input_image_shape)\n",
        "    im_ground_truth = yte[k].reshape(*input_image_shape)\n",
        "\n",
        "    axes[k, 0].set_title(\"Original Test Image\")\n",
        "    axes[k, 0].imshow(im_test_original, cmap='gray')\n",
        "\n",
        "    axes[k, 1].set_title(\"Ground Truth\")\n",
        "    axes[k, 1].imshow(im_ground_truth, cmap='gray')\n",
        "\n",
        "    axes[k, 2].set_title(\"Predicted\")\n",
        "    axes[k, 2].imshow(im_result, cmap='gray')\n",
        "plt.savefig(\"Examples.jpg\", dpi=500)\n",
        "\n",
        "# Show best and worst test example by some metric\n",
        "# best_index = 0\n",
        "# worst_index = 0\n",
        "# best_m = 0\n",
        "# worst_m = 1\n",
        "# for i in range(len(yte)):\n",
        "#     acc_metric.reset_state()\n",
        "#     acc_metric.update_state(imgs_mask_test[i], yte[i])\n",
        "#     m = acc_metric.result().numpy()\n",
        "\n",
        "#     if m < worst_m:\n",
        "#         best_m = m\n",
        "#         worst_index = i\n",
        "\n",
        "#     if m > best_acc:\n",
        "#         best_acc = m\n",
        "#         best_index = i\n",
        "\n",
        "# print(best_index, best_m)\n",
        "# print(worst_index, worst_m)\n",
        "# idxs = [best_index, worst_index]\n",
        "\n",
        "# _, axes = plt.subplots(2, ncols, figsize=(17, 17*2/ncols))\n",
        "# for axis in axes.flatten():\n",
        "#     axis.set_axis_off()\n",
        "#     axis.set_aspect('equal')\n",
        "\n",
        "# for k in range(0, 2):\n",
        "#     im_test_original = Xte[idxs[k]].reshape(*input_image_shape)\n",
        "#     im_result = imgs_mask_test[idxs[k]].reshape(*input_image_shape)\n",
        "#     im_ground_truth = yte[idxs[k]].reshape(*input_image_shape)\n",
        "\n",
        "#     axes[k, 0].set_title(\"Original Test Image\")\n",
        "#     axes[k, 0].imshow(im_test_original, cmap='gray')\n",
        "\n",
        "#     axes[k, 1].set_title(\"Ground Truth\")\n",
        "#     axes[k, 1].imshow(im_ground_truth, cmap='gray')\n",
        "\n",
        "#     axes[k, 2].set_title(\"Predicted\")\n",
        "#     axes[k, 2].imshow(im_result, cmap='gray')"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}